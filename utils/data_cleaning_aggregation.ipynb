{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Multi-Timeframe Aggregation\n",
    "\n",
    "This notebook processes the raw 1-minute BTCUSD dataset by:\n",
    "1. Cleaning the data (removing initial rows with no feature data)\n",
    "2. Imputing remaining NaN values\n",
    "3. Aggregating to multiple timeframes (15min, 30min, 1day)\n",
    "4. Properly handling pre-calculated features during aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Examine Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (484640, 28)\n",
      "Columns: ['Timestamp', 'Unnamed: 0.1', 'Unnamed: 0', 'Open', 'High', 'Low', 'Close', 'Volume', 'ema_7d', 'ema_20d', 'ema_30d', 'sma_7d', 'sma_20d', 'sma_30d', 'macd_12_26', 'macd_sig_12_26', 'macd_hist_12_26', 'rsi_14d', 'bb_mid_20d', 'bb_upper_20d', 'bb_lower_20d', 'atr_14d', 'fd_14d', 'future_close', 'future_return', 'future_trend', 'fd_7d', 'fd_30d']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Timestamp",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Unnamed: 0.1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Unnamed: 0",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Open",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "High",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Low",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Close",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Volume",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ema_7d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ema_20d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ema_30d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sma_7d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sma_20d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sma_30d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "macd_12_26",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "macd_sig_12_26",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "macd_hist_12_26",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rsi_14d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "bb_mid_20d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "bb_upper_20d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "bb_lower_20d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "atr_14d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fd_14d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "future_close",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "future_return",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "future_trend",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fd_7d",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fd_30d",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "b2851379-76d7-4997-8390-2985c9a9244b",
       "rows": [
        [
         "0",
         "2023-01-01 14:00:00",
         "0",
         "0",
         "16474.3",
         "16475.3",
         "16473.1",
         "16475.3",
         "8.79999997778214e-05",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         null,
         "17180.3",
         "0.042791330051653",
         "1",
         null,
         null
        ],
        [
         "1",
         "2023-01-01 14:01:00",
         "1",
         "1",
         "16475.1",
         "16477.7",
         "16474.7",
         "16476.8",
         "8.59999997828709e-05",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         null,
         "17182.3",
         "0.0428177801514857",
         "1",
         null,
         null
        ],
        [
         "2",
         "2023-01-01 14:02:00",
         "2",
         "2",
         "16477.3",
         "16478.2",
         "16476.4",
         "16477.1",
         "8.59999997828709e-05",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         null,
         "17183.2",
         "0.0428534147392443",
         "1",
         null,
         null
        ],
        [
         "3",
         "2023-01-01 14:03:00",
         "3",
         "3",
         "16476.9",
         "16478.5",
         "16476.1",
         "16478.2",
         "9.19999997677223e-05",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         null,
         "17181.4",
         "0.0426745639693656",
         "1",
         null,
         null
        ],
        [
         "4",
         "2023-01-01 14:04:00",
         "4",
         "4",
         "16477.6",
         "16478.9",
         "16477.0",
         "16478.2",
         "6.9999999823267e-05",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         null,
         "17181.4",
         "0.0426745639693656",
         "1",
         null,
         null
        ]
       ],
       "shape": {
        "columns": 28,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>ema_7d</th>\n",
       "      <th>ema_20d</th>\n",
       "      <th>...</th>\n",
       "      <th>bb_mid_20d</th>\n",
       "      <th>bb_upper_20d</th>\n",
       "      <th>bb_lower_20d</th>\n",
       "      <th>atr_14d</th>\n",
       "      <th>fd_14d</th>\n",
       "      <th>future_close</th>\n",
       "      <th>future_return</th>\n",
       "      <th>future_trend</th>\n",
       "      <th>fd_7d</th>\n",
       "      <th>fd_30d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01 14:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16474.3</td>\n",
       "      <td>16475.3</td>\n",
       "      <td>16473.1</td>\n",
       "      <td>16475.3</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17180.3</td>\n",
       "      <td>0.042791</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01 14:01:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16475.1</td>\n",
       "      <td>16477.7</td>\n",
       "      <td>16474.7</td>\n",
       "      <td>16476.8</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17182.3</td>\n",
       "      <td>0.042818</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01 14:02:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16477.3</td>\n",
       "      <td>16478.2</td>\n",
       "      <td>16476.4</td>\n",
       "      <td>16477.1</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17183.2</td>\n",
       "      <td>0.042853</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01 14:03:00</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>16476.9</td>\n",
       "      <td>16478.5</td>\n",
       "      <td>16476.1</td>\n",
       "      <td>16478.2</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17181.4</td>\n",
       "      <td>0.042675</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01 14:04:00</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>16477.6</td>\n",
       "      <td>16478.9</td>\n",
       "      <td>16477.0</td>\n",
       "      <td>16478.2</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17181.4</td>\n",
       "      <td>0.042675</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Timestamp  Unnamed: 0.1  Unnamed: 0     Open     High      Low  \\\n",
       "0  2023-01-01 14:00:00             0           0  16474.3  16475.3  16473.1   \n",
       "1  2023-01-01 14:01:00             1           1  16475.1  16477.7  16474.7   \n",
       "2  2023-01-01 14:02:00             2           2  16477.3  16478.2  16476.4   \n",
       "3  2023-01-01 14:03:00             3           3  16476.9  16478.5  16476.1   \n",
       "4  2023-01-01 14:04:00             4           4  16477.6  16478.9  16477.0   \n",
       "\n",
       "     Close    Volume  ema_7d  ema_20d  ...  bb_mid_20d  bb_upper_20d  \\\n",
       "0  16475.3  0.000088     NaN      NaN  ...         NaN           NaN   \n",
       "1  16476.8  0.000086     NaN      NaN  ...         NaN           NaN   \n",
       "2  16477.1  0.000086     NaN      NaN  ...         NaN           NaN   \n",
       "3  16478.2  0.000092     NaN      NaN  ...         NaN           NaN   \n",
       "4  16478.2  0.000070     NaN      NaN  ...         NaN           NaN   \n",
       "\n",
       "   bb_lower_20d  atr_14d  fd_14d  future_close  future_return  future_trend  \\\n",
       "0           NaN      0.0     NaN       17180.3       0.042791             1   \n",
       "1           NaN      0.0     NaN       17182.3       0.042818             1   \n",
       "2           NaN      0.0     NaN       17183.2       0.042853             1   \n",
       "3           NaN      0.0     NaN       17181.4       0.042675             1   \n",
       "4           NaN      0.0     NaN       17181.4       0.042675             1   \n",
       "\n",
       "   fd_7d  fd_30d  \n",
       "0    NaN     NaN  \n",
       "1    NaN     NaN  \n",
       "2    NaN     NaN  \n",
       "3    NaN     NaN  \n",
       "4    NaN     NaN  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the raw dataset\n",
    "raw_data_path = 'E:\\Coding\\Crypto_Research\\data\\BTCUSD_2023-1min_ML_with_FDs.csv'\n",
    "df = pd.read_csv(raw_data_path)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types:\n",
      "Timestamp           object\n",
      "Unnamed: 0.1         int64\n",
      "Unnamed: 0           int64\n",
      "Open               float64\n",
      "High               float64\n",
      "Low                float64\n",
      "Close              float64\n",
      "Volume             float64\n",
      "ema_7d             float64\n",
      "ema_20d            float64\n",
      "ema_30d            float64\n",
      "sma_7d             float64\n",
      "sma_20d            float64\n",
      "sma_30d            float64\n",
      "macd_12_26         float64\n",
      "macd_sig_12_26     float64\n",
      "macd_hist_12_26    float64\n",
      "rsi_14d            float64\n",
      "bb_mid_20d         float64\n",
      "bb_upper_20d       float64\n",
      "bb_lower_20d       float64\n",
      "atr_14d            float64\n",
      "fd_14d             float64\n",
      "future_close       float64\n",
      "future_return      float64\n",
      "future_trend         int64\n",
      "fd_7d              float64\n",
      "fd_30d             float64\n",
      "dtype: object\n",
      "\n",
      "Missing values per column:\n",
      "ema_7d             10079\n",
      "ema_20d            28799\n",
      "ema_30d            43199\n",
      "sma_7d             10079\n",
      "sma_20d            28799\n",
      "sma_30d            43199\n",
      "macd_12_26         37439\n",
      "macd_sig_12_26     50398\n",
      "macd_hist_12_26    50398\n",
      "rsi_14d            20159\n",
      "bb_mid_20d         28799\n",
      "bb_upper_20d       28799\n",
      "bb_lower_20d       28799\n",
      "fd_14d             20159\n",
      "future_close        7200\n",
      "future_return       7200\n",
      "fd_7d              10079\n",
      "fd_30d             43199\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check data types and missing values\n",
    "print(\"Data types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMissing values per column:\")\n",
    "missing_counts = df.isnull().sum()\n",
    "print(missing_counts[missing_counts > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2023-01-01 14:00:00 to 2023-12-31 14:27:00\n",
      "Total time span: 364 days 00:27:00\n"
     ]
    }
   ],
   "source": [
    "# Convert timestamp column to datetime if needed\n",
    "if 'timestamp' in df.columns:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "elif 'time' in df.columns:\n",
    "    df['timestamp'] = pd.to_datetime(df['time'])\n",
    "    df = df.drop('time', axis=1)\n",
    "elif 'datetime' in df.columns:\n",
    "    df['timestamp'] = pd.to_datetime(df['datetime'])\n",
    "    df = df.drop('datetime', axis=1)\n",
    "else:\n",
    "    # Assume first column is timestamp\n",
    "    timestamp_col = df.columns[0]\n",
    "    df['timestamp'] = pd.to_datetime(df[timestamp_col])\n",
    "    if timestamp_col != 'timestamp':\n",
    "        df = df.drop(timestamp_col, axis=1)\n",
    "\n",
    "# Set timestamp as index\n",
    "df = df.set_index('timestamp')\n",
    "df = df.sort_index()\n",
    "\n",
    "print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "print(f\"Total time span: {df.index.max() - df.index.min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OHLCV columns identified: ['Open', 'High', 'Low', 'Close', 'Volume']\n",
      "Feature columns count: 22\n",
      "First 10 feature columns: ['Unnamed: 0.1', 'Unnamed: 0', 'ema_7d', 'ema_20d', 'ema_30d', 'sma_7d', 'sma_20d', 'sma_30d', 'macd_12_26', 'macd_sig_12_26']\n"
     ]
    }
   ],
   "source": [
    "# Identify OHLCV columns\n",
    "ohlcv_cols = []\n",
    "for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "    if col in df.columns:\n",
    "        ohlcv_cols.append(col)\n",
    "    elif col.upper() in df.columns:\n",
    "        ohlcv_cols.append(col.upper())\n",
    "    elif col.capitalize() in df.columns:\n",
    "        ohlcv_cols.append(col.capitalize())\n",
    "\n",
    "print(f\"OHLCV columns identified: {ohlcv_cols}\")\n",
    "\n",
    "# Identify feature columns (everything except OHLCV)\n",
    "feature_cols = [col for col in df.columns if col not in ohlcv_cols]\n",
    "print(f\"Feature columns count: {len(feature_cols)}\")\n",
    "print(f\"First 10 feature columns: {feature_cols[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First valid feature data starts at: 2023-02-10 03:16:00\n",
      "Removing data before: 2023-02-10\n",
      "Original dataset shape: (484640, 27)\n",
      "Cleaned dataset shape: (434438, 27)\n",
      "Removed 50202 rows\n"
     ]
    }
   ],
   "source": [
    "# Find the first day where features have data\n",
    "first_valid_feature_idx = df[feature_cols].dropna().index[0] if len(df[feature_cols].dropna()) > 0 else df.index[0]\n",
    "first_day = first_valid_feature_idx.date()\n",
    "\n",
    "print(f\"First valid feature data starts at: {first_valid_feature_idx}\")\n",
    "print(f\"Removing data before: {first_day}\")\n",
    "\n",
    "# Remove data before first valid feature day\n",
    "df_cleaned = df[df.index.date >= first_day].copy()\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Cleaned dataset shape: {df_cleaned.shape}\")\n",
    "print(f\"Removed {df.shape[0] - df_cleaned.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Impute Remaining NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after cleaning:\n",
      "macd_sig_12_26      196\n",
      "macd_hist_12_26     196\n",
      "future_close       7200\n",
      "future_return      7200\n",
      "dtype: int64\n",
      "\n",
      "No missing values remaining after imputation\n"
     ]
    }
   ],
   "source": [
    "# Check remaining missing values\n",
    "missing_after_cleaning = df_cleaned.isnull().sum()\n",
    "print(\"Missing values after cleaning:\")\n",
    "print(missing_after_cleaning[missing_after_cleaning > 0])\n",
    "\n",
    "# Impute missing values\n",
    "df_imputed = df_cleaned.copy()\n",
    "\n",
    "# For OHLCV data, use forward fill then backward fill\n",
    "for col in ohlcv_cols:\n",
    "    if col in df_imputed.columns:\n",
    "        df_imputed[col] = df_imputed[col].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# For feature columns, use forward fill then backward fill\n",
    "for col in feature_cols:\n",
    "    if col in df_imputed.columns:\n",
    "        df_imputed[col] = df_imputed[col].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Final check for any remaining NaN values\n",
    "final_missing = df_imputed.isnull().sum()\n",
    "if final_missing.sum() > 0:\n",
    "    print(\"\\nRemaining missing values after imputation:\")\n",
    "    print(final_missing[final_missing > 0])\n",
    "    # Fill any remaining NaN with 0\n",
    "    df_imputed = df_imputed.fillna(0)\n",
    "else:\n",
    "    print(\"\\nNo missing values remaining after imputation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Aggregation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation functions defined\n"
     ]
    }
   ],
   "source": [
    "def create_aggregation_dict(df, ohlcv_cols, feature_cols):\n",
    "    \"\"\"\n",
    "    Create aggregation dictionary for resampling.\n",
    "    OHLCV gets standard OHLCV aggregation.\n",
    "    Features get last value (since they're already calculated indicators).\n",
    "    \"\"\"\n",
    "    agg_dict = {}\n",
    "    \n",
    "    # Standard OHLCV aggregation\n",
    "    for col in ohlcv_cols:\n",
    "        if col in df.columns:\n",
    "            if col.lower() in ['open']:\n",
    "                agg_dict[col] = 'first'\n",
    "            elif col.lower() in ['high']:\n",
    "                agg_dict[col] = 'max'\n",
    "            elif col.lower() in ['low']:\n",
    "                agg_dict[col] = 'min'\n",
    "            elif col.lower() in ['close']:\n",
    "                agg_dict[col] = 'last'\n",
    "            elif col.lower() in ['volume']:\n",
    "                agg_dict[col] = 'sum'\n",
    "    \n",
    "    # Features: use last value since they're pre-calculated indicators\n",
    "    for col in feature_cols:\n",
    "        if col in df.columns:\n",
    "            agg_dict[col] = 'last'\n",
    "    \n",
    "    return agg_dict\n",
    "\n",
    "def aggregate_timeframe(df, timeframe, ohlcv_cols, feature_cols):\n",
    "    \"\"\"\n",
    "    Aggregate dataframe to specified timeframe.\n",
    "    \"\"\"\n",
    "    agg_dict = create_aggregation_dict(df, ohlcv_cols, feature_cols)\n",
    "    \n",
    "    # Resample to target timeframe\n",
    "    df_agg = df.resample(timeframe).agg(agg_dict)\n",
    "    \n",
    "    # Remove rows where OHLCV data is missing (market closed periods)\n",
    "    if ohlcv_cols:\n",
    "        df_agg = df_agg.dropna(subset=[col for col in ohlcv_cols if col in df_agg.columns])\n",
    "    \n",
    "    return df_agg\n",
    "\n",
    "print(\"Aggregation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Multi-Timeframe Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 15-minute dataset...\n",
      "15-minute dataset shape: (30925, 27)\n"
     ]
    }
   ],
   "source": [
    "# Create 15-minute dataset\n",
    "print(\"Creating 15-minute dataset...\")\n",
    "df_15min = aggregate_timeframe(df_imputed, '15T', ohlcv_cols, feature_cols)\n",
    "print(f\"15-minute dataset shape: {df_15min.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 30-minute dataset...\n",
      "30-minute dataset shape: (15466, 27)\n"
     ]
    }
   ],
   "source": [
    "# Create 30-minute dataset\n",
    "print(\"Creating 30-minute dataset...\")\n",
    "df_30min = aggregate_timeframe(df_imputed, '30T', ohlcv_cols, feature_cols)\n",
    "print(f\"30-minute dataset shape: {df_30min.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 1-hour dataset...\n",
      "1-hour dataset shape: (7738, 27)\n"
     ]
    }
   ],
   "source": [
    "# Create 1-hour dataset\n",
    "print(\"Creating 1-hour dataset...\")\n",
    "df_1h = aggregate_timeframe(df_imputed, '1H', ohlcv_cols, feature_cols)\n",
    "print(f\"1-hour dataset shape: {df_1h.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating daily dataset...\n",
      "Daily dataset shape: (325, 27)\n"
     ]
    }
   ],
   "source": [
    "# Create daily dataset\n",
    "print(\"Creating daily dataset...\")\n",
    "df_daily = aggregate_timeframe(df_imputed, '1D', ohlcv_cols, feature_cols)\n",
    "print(f\"Daily dataset shape: {df_daily.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset summary:\n",
      "\n",
      "1min:\n",
      "  Shape: (434438, 27)\n",
      "  Date range: 2023-02-10 00:00:00 to 2023-12-31 14:27:00\n",
      "  Missing values: 0\n",
      "  Infinite values: 0\n",
      "\n",
      "15min:\n",
      "  Shape: (30925, 27)\n",
      "  Date range: 2023-02-10 00:00:00 to 2023-12-31 14:15:00\n",
      "  Missing values: 0\n",
      "  Infinite values: 0\n",
      "\n",
      "30min:\n",
      "  Shape: (15466, 27)\n",
      "  Date range: 2023-02-10 00:00:00 to 2023-12-31 14:00:00\n",
      "  Missing values: 0\n",
      "  Infinite values: 0\n",
      "\n",
      "1h:\n",
      "  Shape: (7738, 27)\n",
      "  Date range: 2023-02-10 00:00:00 to 2023-12-31 14:00:00\n",
      "  Missing values: 0\n",
      "  Infinite values: 0\n",
      "\n",
      "1day:\n",
      "  Shape: (325, 27)\n",
      "  Date range: 2023-02-10 00:00:00 to 2023-12-31 00:00:00\n",
      "  Missing values: 0\n",
      "  Infinite values: 0\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    '1min': df_imputed,\n",
    "    '15min': df_15min,\n",
    "    '30min': df_30min,\n",
    "    '1h': df_1h,\n",
    "    '1day': df_daily\n",
    "}\n",
    "\n",
    "print(\"Dataset summary:\")\n",
    "for name, data in datasets.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Shape: {data.shape}\")\n",
    "    print(f\"  Date range: {data.index.min()} to {data.index.max()}\")\n",
    "    print(f\"  Missing values: {data.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Check for any infinite values\n",
    "    inf_count = np.isinf(data.select_dtypes(include=[np.number])).sum().sum()\n",
    "    print(f\"  Infinite values: {inf_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Processed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1min dataset to: ../data\\BTCUSD_2023_1min_cleaned.csv\n",
      "  File size: 177.78 MB\n",
      "Saved 15min dataset to: ../data\\BTCUSD_2023_15min_cleaned.csv\n",
      "  File size: 12.87 MB\n",
      "Saved 30min dataset to: ../data\\BTCUSD_2023_30min_cleaned.csv\n",
      "  File size: 6.43 MB\n",
      "Saved 1h dataset to: ../data\\BTCUSD_2023_1h_cleaned.csv\n",
      "  File size: 3.22 MB\n",
      "Saved 1day dataset to: ../data\\BTCUSD_2023_1day_cleaned.csv\n",
      "  File size: 0.13 MB\n",
      "\n",
      "All datasets saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "data_dir = '../data'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Save datasets\n",
    "save_paths = {}\n",
    "\n",
    "for timeframe, data in datasets.items():\n",
    "    filename = f'BTCUSD_2023_{timeframe}_cleaned.csv'\n",
    "    filepath = os.path.join(data_dir, filename)\n",
    "    \n",
    "    # Reset index to save timestamp as column\n",
    "    data_to_save = data.reset_index()\n",
    "    data_to_save.to_csv(filepath, index=False)\n",
    "    \n",
    "    save_paths[timeframe] = filepath\n",
    "    print(f\"Saved {timeframe} dataset to: {filepath}\")\n",
    "    print(f\"  File size: {os.path.getsize(filepath) / (1024*1024):.2f} MB\")\n",
    "\n",
    "print(\"\\nAll datasets saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Summary:\n",
      "Original dataset: 484,640 rows\n",
      "After cleaning: 434,438 rows\n",
      "Reduction: 50,202 rows removed\n",
      "\n",
      "Aggregated datasets:\n",
      "15min: 30,925 rows (compression: 14.0x)\n",
      "30min: 15,466 rows (compression: 28.1x)\n",
      "1h: 7,738 rows (compression: 56.1x)\n",
      "1day: 325 rows (compression: 1336.7x)\n",
      "\n",
      "Feature preservation check:\n",
      "Original features: 22\n",
      "1min: 22 features preserved\n",
      "15min: 22 features preserved\n",
      "30min: 22 features preserved\n",
      "1h: 22 features preserved\n",
      "1day: 22 features preserved\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing Summary:\")\n",
    "print(f\"Original dataset: {df.shape[0]:,} rows\")\n",
    "print(f\"After cleaning: {df_imputed.shape[0]:,} rows\")\n",
    "print(f\"Reduction: {df.shape[0] - df_imputed.shape[0]:,} rows removed\")\n",
    "\n",
    "print(\"\\nAggregated datasets:\")\n",
    "for timeframe, data in datasets.items():\n",
    "    if timeframe != '1min':\n",
    "        compression_ratio = df_imputed.shape[0] / data.shape[0]\n",
    "        print(f\"{timeframe}: {data.shape[0]:,} rows (compression: {compression_ratio:.1f}x)\")\n",
    "\n",
    "print(\"\\nFeature preservation check:\")\n",
    "print(f\"Original features: {len(feature_cols)}\")\n",
    "for timeframe, data in datasets.items():\n",
    "    preserved_features = len([col for col in feature_cols if col in data.columns])\n",
    "    print(f\"{timeframe}: {preserved_features} features preserved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Crypto_Research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
